{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignment1 COMP90049\n",
    "\n",
    "'''\n",
    "\n",
    "Date created: 03/27/2018\n",
    "\n",
    "Date last modified: 04/11/2018\n",
    "\n",
    "Python Version: 2.7\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy, Precision and Recall\n",
    "\n",
    "The following code implements three functions, each for calculating accuracy, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted_list, correct_list):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    while(i < len(predicted_list)):\n",
    "        if predicted_list[i] == correct_list[i]:\n",
    "            count += 1\n",
    "        i += 1\n",
    "    return count / float(len(predicted_list))\n",
    "\n",
    "def calculate_precision(predicted_list, correct_list):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    total = 0\n",
    "\n",
    "    while(i < len(predicted_list)):\n",
    "        if correct_list[i] in predicted_list[i]:\n",
    "            count += 1\n",
    "        total += len(predicted_list[i])\n",
    "        i += 1\n",
    "    return count / float(total)\n",
    "\n",
    "def calculate_recall(predicted_list, correct_list):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    while(i < len(predicted_list)):\n",
    "        if correct_list[i] in predicted_list[i]:\n",
    "            count += 1\n",
    "        i += 1\n",
    "\n",
    "    return count / float(len(correct_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Edit Distance\n",
    "In the following code global edit distance search is implemented by calculating Levenshtein Distance. A list of sets containing predicted words is retrieved \"predicted_list\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "dictionary_set = set(record.strip() for record in open('dictionary.txt'))\n",
    "misspell_list = []\n",
    "correct_list = []\n",
    "predicted_list = []\n",
    "\n",
    "with open(\"correct.txt\") as record :\n",
    "    for line in record:\n",
    "        word = line.strip()\n",
    "        correct_list.append(word)\n",
    "        \n",
    "with open(\"misspell.txt\") as record :\n",
    "    for line in record:\n",
    "        word = line.strip()\n",
    "        misspell_list.append(word)\n",
    "        if word in dictionary_set :\n",
    "            predicted_list.append(set([word]))\n",
    "        else:  \n",
    "            minimum = float(\"inf\")\n",
    "            predicted_words = set()\n",
    "            for word_dict in dictionary_set:\n",
    "                temp_stor = editdistance.eval(word, word_dict)\n",
    "                if temp_stor < minimum :\n",
    "                    predicted_words.clear()\n",
    "                    predicted_words.add(word_dict)\n",
    "                    minimum = temp_stor\n",
    "                elif temp_stor == minimum :\n",
    "                    predicted_words.add(word_dict)\n",
    "            predicted_list.append(predicted_words)\n",
    "end = time.time()\n",
    "time_spent = (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GED - Levenshtein Distance (Precision and Recall Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "The precision-score of the method used is\n",
      "0.0457670043415\n",
      "------------------\n",
      "The recall-score of the method used is\n",
      "0.353351955307\n",
      "------------------\n",
      "The time spent in execution of code is\n",
      "314.933781862\n"
     ]
    }
   ],
   "source": [
    "print \"------------------\"\n",
    "print \"The precision-score of the method used is\"\n",
    "print calculate_precision(predicted_list, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method used is\"\n",
    "print calculate_recall(predicted_list, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The time spent in execution of code is\"\n",
    "print time_spent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neighbourhood Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def neighbour_generator(word, k):\n",
    "    predicted_words = set()\n",
    "    for word_dict in dictionary_set:\n",
    "        temp_stor = editdistance.eval(word, word_dict)\n",
    "        if temp_stor <= k :\n",
    "            predicted_words.add(word_dict)\n",
    "    return predicted_words\n",
    "    \n",
    "predicted_list_k1 = []\n",
    "    \n",
    "for word in misspell_list:\n",
    "    predicted_words = neighbour_generator(word, 1)\n",
    "    predicted_list_k1.append(predicted_words)\n",
    "    \n",
    "predicted_list_k2 = []\n",
    "    \n",
    "for word in misspell_list:\n",
    "    predicted_words = neighbour_generator(word, 2)\n",
    "    predicted_list_k2.append(predicted_words)\n",
    "\n",
    "predicted_list_k3 = []\n",
    "    \n",
    "for word in misspell_list:\n",
    "    predicted_words = neighbour_generator(word, 3)\n",
    "    predicted_list_k3.append(predicted_words)\n",
    "    \n",
    "predicted_list_k4 = []\n",
    "    \n",
    "for word in misspell_list:\n",
    "    predicted_words = neighbour_generator(word, 4)\n",
    "    predicted_list_k4.append(predicted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The precision and recall score based of different values of k in neighbourhood search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "The precision-score of the method when k = 1 is\n",
      "0.0438121047877\n",
      "------------------\n",
      "The recall-score of the method when k = 1 is\n",
      "0.406424581006\n",
      "------------------\n",
      "------------------\n",
      "The precision-score of the method when k = 2 is\n",
      "0.00353290408897\n",
      "------------------\n",
      "The recall-score of the method when k = 2 is\n",
      "0.708100558659\n",
      "------------------\n",
      "------------------\n",
      "The precision-score of the method when k = 3 is\n",
      "0.000349547267871\n",
      "------------------\n",
      "The recall-score of the method when k = 3 is\n",
      "0.784916201117\n",
      "------------------\n",
      "------------------\n",
      "The precision-score of the method when k = 4 is\n",
      "6.54787652589e-05\n",
      "------------------\n",
      "The recall-score of the method when k = 4 is\n",
      "0.815642458101\n"
     ]
    }
   ],
   "source": [
    "print \"------------------\"\n",
    "print \"The precision-score of the method when k = 1 is\"\n",
    "print calculate_precision(predicted_list_k1, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method when k = 1 is\"\n",
    "print calculate_recall(predicted_list_k1, correct_list)\n",
    "print \"------------------\"\n",
    "print \"------------------\"\n",
    "print \"The precision-score of the method when k = 2 is\"\n",
    "print calculate_precision(predicted_list_k2, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method when k = 2 is\"\n",
    "print calculate_recall(predicted_list_k2, correct_list)\n",
    "print \"------------------\"\n",
    "print \"------------------\"\n",
    "print \"The precision-score of the method when k = 3 is\"\n",
    "print calculate_precision(predicted_list_k3, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method when k = 3 is\"\n",
    "print calculate_recall(predicted_list_k3, correct_list)\n",
    "print \"------------------\"\n",
    "print \"------------------\"\n",
    "print \"The precision-score of the method when k = 4 is\"\n",
    "print calculate_precision(predicted_list_k4, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method when k = 4 is\"\n",
    "print calculate_recall(predicted_list_k4, correct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-Gram Distance Method\n",
    "\n",
    "The following function implements n-gram prediction method. It takes a misspelled word and N-value as argument and returns a set of predicted words,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngram\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def ngram_predict(*args):\n",
    "    word = args[0][0]\n",
    "    n_val = args[0][1]\n",
    "    predicted_words = set()\n",
    "    maximum = -1\n",
    "    for word_dict in dictionary_set:\n",
    "        temp_stor = ngram.NGram.compare(word, word_dict, N = n_val)\n",
    "        if temp_stor > maximum :\n",
    "            predicted_words.clear()\n",
    "            predicted_words.add(word_dict)\n",
    "            maximum = temp_stor\n",
    "        elif temp_stor == maximum :\n",
    "            predicted_words.add(word_dict)\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 26 Jun. 2018\n",
    "\n",
    "@author: Anubhav Singh\n",
    "'''\n",
    "import sys\n",
    "import json\n",
    "import os.path\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "import copyreg\n",
    "import types\n",
    "import stanfordcorenlp\n",
    "\n",
    "def _pickle_method(method):\n",
    "    func_name = method.im_func.__name__\n",
    "    obj = method.im_self\n",
    "    cls = method.im_class\n",
    "    if func_name.startswith('__') and not func_name.endswith('__'): #deal with mangled names\n",
    "        cls_name = cls.__name__.lstrip('_')\n",
    "        func_name = '_' + cls_name + func_name\n",
    "    return _unpickle_method, (func_name, obj, cls)\n",
    "\n",
    "def _unpickle_method(func_name, obj, cls):\n",
    "    for cls in cls.__mro__:\n",
    "        try:\n",
    "            func = cls.__dict__[func_name]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "    return func.__get__(obj, cls)\n",
    "\n",
    "\n",
    "copyreg.pickle(types.MethodType, _pickle_method, _unpickle_method)\n",
    "\n",
    "#===============================================================================\n",
    "# \n",
    "#===============================================================================\n",
    "class Documents(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "    def __init__(self,corenlp=None):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "\n",
    "        self.corenlp = corenlp\n",
    "        #------------------------------------------------ Load the dataset files\n",
    "        with open('/home/xoxo/eclipse-workspace/zUra/dataset/documents.json') as f:\n",
    "            self.documents_json = json.load(f)\n",
    "\n",
    "#===============================================================================\n",
    "# \n",
    "#===============================================================================\n",
    "    def generate_tagged_corpus_stanfordnlp(self):\n",
    "        p = Pool(4)\n",
    "        self.documents_json = p.map(self.tokenize_n_tag_stanfordnlp, [1,2,3])\n",
    "\n",
    "#===============================================================================\n",
    "# \n",
    "#===============================================================================\n",
    "    def tokenize_n_tag_stanfordnlp(self, doc):\n",
    "        print(doc)\n",
    "\n",
    "\n",
    "\n",
    "corenlp = stanfordcorenlp.StanfordCoreNLP(r'/home/xoxo/eclipse-workspace/zUra/src/stanford-corenlp-full-2018-02-27')\n",
    "document_obj = Documents(corenlp)\n",
    "document_obj.generate_tagged_corpus_stanfordnlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-gram with n = 2, Precision and Recall Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"------------------\")? (<ipython-input-2-faaeca3b46b4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-faaeca3b46b4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print \"------------------\"\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"------------------\")?\n"
     ]
    }
   ],
   "source": [
    "print \"------------------\"\n",
    "print \"The precision-score of the method used is\"\n",
    "print calculate_precision(predicted_list_ngram2, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method used is\"\n",
    "print calculate_recall(predicted_list_ngram2, correct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(4)\n",
    "\n",
    "args_list = []\n",
    "for word in misspell_list:\n",
    "    args_list.append((word,3))\n",
    "predicted_list_ngram3 = p.map(ngram_predict,args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N-gram with n = 3, Precision and Recall Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "The precision-score of the method used is\n",
      "0.126829268293\n",
      "------------------\n",
      "The recall-score of the method used is\n",
      "0.18156424581\n"
     ]
    }
   ],
   "source": [
    "print \"------------------\"\n",
    "print \"The precision-score of the method used is\"\n",
    "print calculate_precision(predicted_list_ngram3, correct_list)\n",
    "print \"------------------\"\n",
    "print \"The recall-score of the method used is\"\n",
    "print calculate_recall(predicted_list_ngram3, correct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
